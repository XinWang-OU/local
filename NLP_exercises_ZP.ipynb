{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e63007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you have any errors, do the following: \n",
    "#conda activate NLP_py38 \n",
    "#cde data download \n",
    "#The first step is to collect data. Here we take a one-sentence example to \n",
    "#demonstrate the simplest text preprocessing before model training.\n",
    "\n",
    "\n",
    "#one of the key concept in NLP is tokenize, i.e., split sentences into words.\n",
    "#This cell and the following will show how this is achieved with gensim and a \n",
    "#toolkit that I developed.\n",
    "\n",
    "from alloy2vec.processing import MaterialsTextProcessor\n",
    "text_processor = MaterialsTextProcessor()\n",
    "text_processor.process(\"New York University is one of the best universities in the world.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6146a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentences directly using gensim\n",
    "\n",
    "from gensim.utils import tokenize\n",
    "list(tokenize(\"New York University is one of the best universities in the world.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf28f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words, like \"is\", \"one\", \"the\", \"of\", are not really useful when included in\n",
    "#the training dataset.\n",
    "#remove the stop words \n",
    "from gensim.parsing.preprocessing import remove_stopwords,strip_punctuation\n",
    "remove_stopwords(\"New York University is one of the best universities in the world.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f591535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarly, punctuations like \",\", \".\", are also not important to include.\n",
    "\n",
    "words_no_stopwords=remove_stopwords(\"New York University is one of the best universities in the world.\")\n",
    "strip_punctuation(words_no_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27e476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at the tokenized words again after text preprocessed.\n",
    "\n",
    "words_no_stopwords_punctuation=strip_punctuation(words_no_stopwords)\n",
    "text_processor.process(words_no_stopwords_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c1b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we skip the time-consuming data training\n",
    "# Let's try on one model that I trained.\n",
    "\n",
    "#load the model. May take a while, given the model's large size.\n",
    "from gensim.models import Word2Vec \n",
    "w2v_model =Word2Vec.load(\"alloy2vec/training/models/model_121520\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c6bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's have a look at the one-dimentional word vector and its dimensional size\n",
    "\n",
    "word=\"excellent\"\n",
    "print(\"word vector of \"+str(word)+\" :\", w2v_model.wv.get_vector(word))\n",
    "print(\"dimension size:\", len(w2v_model.wv.get_vector(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f5a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one simple example to check the most similar words of \"excellent\"\n",
    "# try different words if you like. If the word is not included in the\n",
    "# vocabulary, it may complains. Then, try a different one.\n",
    "word=\"excellent\"\n",
    "w2v_model.wv.most_similar(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f679f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "a=w2v_model.wv.most_similar(word,topn=20) #200\n",
    "word_candidates,cosine_similarity=[],[]\n",
    "print(a)\n",
    "for i in range(0,20): #len(a)):\n",
    "  word_candidates.append(a[i*1][0])\n",
    "  cosine_similarity.append(a[i*1][1])\n",
    "word_num=np.arange(len(word_candidates))\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(word_num, cosine_similarity,color='blue', align='center') #color='#0504aa',\n",
    "ax.set_yticks(word_num)\n",
    "ax.set_yticklabels(word_candidates)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "plt.xlim((0.4,0.95))\n",
    "ax.set_xlabel('cosine similarity')\n",
    "ax.set_title('Ranking of cosine similarity for'+' \"'+word+'\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try some chemical concept:\n",
    "# the full name of chemical elements.\n",
    "\n",
    "w2v_model.wv.most_similar(\n",
    "    positive=[\"magnesium\", \"Fe\"], \n",
    "    negative=[\"Mg\"], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac4847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:text-mining]",
   "language": "python",
   "name": "conda-env-text-mining-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
